{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mlflow\n",
    "import datetime\n",
    "import pandas as pd \n",
    "import collections\n",
    "import numpy as np\n",
    "from sklearn.metrics import f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get mlflow runs\n",
    "experiment_id = '673705349750158929' # TODO update mlflow experiment ID if it changes (check mlruns directory)\n",
    "\n",
    "mlflow.set_tracking_uri(\"../../mlruns\")\n",
    "\n",
    "runs = mlflow.search_runs(experiment_ids=[experiment_id])\n",
    "\n",
    "failed_runs = len(runs[runs['status']=='FAILED'][['params.model']])\n",
    "print(\"{} experiment runs failed ({}% of total)\".format(failed_runs, failed_runs/len(runs)*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# timestamp for saving figures, tables and other outputs from this experiment run\n",
    "timestamp = datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "print(timestamp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "runs.to_csv(f'results-main-{timestamp}.csv', index=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cleanup the data fields for analysis\n",
    "runs['C'] = runs['params.datafile'].str.split('_').str[2]\n",
    "runs['trial'] = runs['params.datafile'].str.split('_').str[4]\n",
    "runs['params.causal_distance'] = runs['params.causal_distance'].fillna('')\n",
    "runs['params.inference_type'] = runs['params.inference_type'].fillna('')\n",
    "runs['method'] = runs['params.model'] + runs['params.causal_distance'] + runs['params.inference_type']\n",
    "\n",
    "# get best result for each trial, based on performance for validation set\n",
    "results = runs[['method','C','trial','metrics.RMSE_avg_val','metrics.RMSE_avg_test']].sort_values(by='metrics.RMSE_avg_val').groupby(['method','C','trial']).first().reset_index()\n",
    "results['C'] = results['C'].astype(float)\n",
    "results['metrics.RMSE_avg_test'] = results['metrics.RMSE_avg_test'].astype(float)\n",
    "\n",
    "method_names = {'global_bnn_baseline':'Global BNN baseline', \n",
    "                'individual_bnn_baseline':'Local BNNs baseline', \n",
    "                'bayesian_maml_baseline':'Meta-learning baseline',\n",
    "                'our_methodground_truth':'Ground truth reference',\n",
    "                'our_methodOD':'Our method (OD)',\n",
    "                'our_methodSHD':'Our method (SHD)',\n",
    "                'our_methodID':'Our method (ID)',\n",
    "                'our_methodSID':'Our method (SID)',\n",
    "                'our_methodobservational':'Our method (OP)',\n",
    "                'our_methodinterventional':'Our method (IP)'}\n",
    "results['method'] = results['method'].map(method_names)\n",
    "\n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create plot\n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as ticker\n",
    "\n",
    "sns.set_style(\"whitegrid\")\n",
    "\n",
    "fig, axes = plt.subplots(1, 3, figsize=(16,4))\n",
    "\n",
    "methods1 = ['Global BNN baseline', 'Local BNNs baseline', 'Meta-learning baseline', 'Our method (OD)', 'Our method (SHD)', 'Our method (ID)', 'Our method (SID)']\n",
    "methods2 = ['Global BNN baseline', 'Local BNNs baseline', 'Meta-learning baseline', 'Our method (OP)', 'Our method (IP)']\n",
    "methods3 = ['Ground truth reference', 'Our method (OD)', 'Our method (SHD)', 'Our method (ID)', 'Our method (SID)', 'Our method (OP)', 'Our method (IP)']\n",
    "\n",
    "colors = {'Global BNN baseline':'tab:blue', \n",
    "          'Local BNNs baseline':'tab:pink', \n",
    "          'Meta-learning baseline':'tab:olive', \n",
    "          'Ground truth reference':'tab:red', \n",
    "          'Our method (OD)':'tab:cyan', \n",
    "          'Our method (SHD)':'tab:brown', \n",
    "          'Our method (ID)':'tab:orange', \n",
    "          'Our method (SID)':'tab:gray', \n",
    "          'Our method (OP)':'tab:green', \n",
    "          'Our method (IP)':'tab:purple'}\n",
    "\n",
    "colors1 = {m:colors[m] for m in methods1}\n",
    "colors2 = {m:colors[m] for m in methods2}\n",
    "colors3 = {m:colors[m] for m in methods3}\n",
    "\n",
    "ax1 = sns.lineplot(data=results[results['method'].isin(methods1)], x='C', y='metrics.RMSE_avg_test', hue='method', style='method', ax=axes[0], palette=colors1)\n",
    "ax2 = sns.lineplot(data=results[results['method'].isin(methods2)], x='C', y='metrics.RMSE_avg_test', hue='method', style='method', ax=axes[1], palette=colors2)\n",
    "ax3 = sns.lineplot(data=results[results['method'].isin(methods3)], x='C', y='metrics.RMSE_avg_test', hue='method', style='method', ax=axes[2], palette=colors3)\n",
    "\n",
    "sns.despine(left=True)\n",
    "ax1.set_ylabel('RMSE of test tasks')\n",
    "ax1.set_yscale('log')\n",
    "ax1.yaxis.set_major_locator(ticker.LogLocator(10,[0.01,0.02,0.03]))\n",
    "ax1.yaxis.set_major_formatter(ticker.ScalarFormatter())\n",
    "ax2.set_ylabel('RMSE of test tasks')\n",
    "ax2.set_yscale('log')\n",
    "ax2.yaxis.set_major_locator(ticker.LogLocator(10,[0.01,0.02,0.03]))\n",
    "ax2.yaxis.set_major_formatter(ticker.ScalarFormatter())\n",
    "ax3.set_ylabel('RMSE of test tasks')\n",
    "ax1.set_title('Baseline comparison (known CGMs)')\n",
    "ax2.set_title('Baseline comparison (unknown CGMs)')\n",
    "ax3.set_title('Causal distance and proxy comparison')\n",
    "\n",
    "ax1.legend().set_title('')\n",
    "ax2.legend().set_title('')\n",
    "ax3.legend().set_title('')\n",
    "\n",
    "for ax in axes:\n",
    "    ax.legend(loc='upper center', bbox_to_anchor=(0.5, -0.15), ncol=2, borderpad=0.1, columnspacing=0.5)\n",
    "\n",
    "plt.savefig(f'results-main2-{timestamp}.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate percentage difference in performance between meta-learning baseline and other causal distances\n",
    "tmp = results[results['method'].isin(['Meta-learning baseline','Our method (SHD)','Our method (SID)','Our method (OD)','Our method (ID)','Our method (OP)','Our method (IP)'])]\n",
    "tmp['key'] = tmp['trial'].astype(str) + ':' + tmp['C'].astype(str)\n",
    "tmp = tmp.pivot(index='key',columns='method',values='metrics.RMSE_avg_test').reset_index()\n",
    "for method in ['Our method (SHD)','Our method (SID)','Our method (OD)','Our method (ID)','Our method (OP)','Our method (IP)']:\n",
    "    tmp[method] = (tmp[method] - tmp['Meta-learning baseline'])/tmp['Meta-learning baseline']*100 \n",
    "tmp['C'] = tmp['key'].str.split(':').str[1]\n",
    "tmp = tmp.drop(columns=['Meta-learning baseline'])\n",
    "tmp.groupby('C').mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Accuracy of causal group assignments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate accuracy for each trial (model selection using hyperparameter tuning on validation set)\n",
    "runs['causal_dist'] = runs['params.causal_distance'] + runs['params.inference_type']\n",
    "results = runs[['method','C','trial','causal_dist','metrics.RMSE_avg_val','metrics.RMSE_avg_test','params.outprefix','params.datafile']].sort_values(by='metrics.RMSE_avg_val').groupby(['method','C','trial']).first().reset_index()\n",
    "results['C'] = results['C'].astype(float)\n",
    "results['metrics.RMSE_avg_test'] = results['metrics.RMSE_avg_test'].astype(float)\n",
    "results = results[results['method'].str.startswith('our_method')]\n",
    "results = results[~(results['causal_dist']=='ground_truth')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def match_ground_truth_label(labels, ground_truth_labels, num_causal_groups):\n",
    "    \"\"\"Match the cluster labels, according to which ground truth label is most prevalent in each (inferred) cluster\n",
    "    \"\"\"\n",
    "    # calculate how much of each ground truth label in each inferred cluster\n",
    "    freq_data = []\n",
    "    for pred_label in range(num_causal_groups):\n",
    "        count_gt = ground_truth_labels[np.where(labels==pred_label)] \n",
    "        freq_gt = collections.Counter(count_gt)\n",
    "        for gt_label in freq_gt:\n",
    "            freq_data.append({'pred':pred_label, 'gt':gt_label, 'count':freq_gt[gt_label], 'freq':freq_gt[gt_label]/len(count_gt)})\n",
    "\n",
    "    # match a ground truth label to each inferred cluster,\n",
    "    # based on the most common label in a cluster\n",
    "    correction = np.zeros(num_causal_groups)\n",
    "    cluster_df = pd.DataFrame(freq_data).sort_values(by='freq', ascending=False)\n",
    "    assigned_gt = [] \n",
    "    assigned_pred = []\n",
    "    for idx, row in cluster_df.iterrows():\n",
    "        # check if ground truth or inferred label have already been assigned \n",
    "        # (give priority to cluster with highest % of a ground truth label)\n",
    "        if row['gt'] not in assigned_gt and row['pred'] not in assigned_pred: \n",
    "            correction[int(row['pred'])] = int(row['gt'])\n",
    "            assigned_gt.append(row['gt'])\n",
    "            assigned_pred.append(row['pred'])\n",
    "\n",
    "    return correction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = []\n",
    "\n",
    "results['casualassignment_path'] = '../../' + results['params.outprefix'] + '_causal_assignments.csv'\n",
    "results['groundtruth_path'] = results['params.datafile'].str[0:-8] + 'task_metadata.csv'\n",
    "\n",
    "for idx, row in results.iterrows():\n",
    "    causalassignment_path = row['casualassignment_path']\n",
    "    groundtruth_path = row['groundtruth_path']\n",
    "    num_groups = int(row['C'])\n",
    "    method = row['causal_dist']\n",
    "    df = pd.read_csv(causalassignment_path)\n",
    "    df_gt = pd.read_csv(groundtruth_path)\n",
    "\n",
    "    pred_groups = np.array(df['predicted_groups'].tolist())\n",
    "    gt_groups = np.array(df_gt['ground_truth'].tolist())\n",
    "\n",
    "    assert len(pred_groups)==len(gt_groups)\n",
    "\n",
    "    # label alignment function (only use train data for this)\n",
    "    pred_groups_train = pred_groups[0:200]\n",
    "    gt_groups_train = gt_groups[0:200]\n",
    "    correction = match_ground_truth_label(pred_groups_train, gt_groups_train, num_groups)\n",
    "    pred_groups = np.choose(pred_groups,correction)\n",
    "\n",
    "    # evaluate accuracy with f1 score\n",
    "    # get overall score, and also stratify into 1. train and 2. val+test\n",
    "    f1_overall = f1_score(gt_groups, pred_groups, average='macro')\n",
    "    f1_train = f1_score(gt_groups[0:200], pred_groups[0:200], average='macro')\n",
    "    f1_val_test = f1_score(gt_groups[200:], pred_groups[200:], average='macro')\n",
    "\n",
    "    data.append({'C':num_groups, 'method':method, 'metric':'f1_overall', 'f1':f1_overall})\n",
    "    data.append({'C':num_groups, 'method':method, 'metric':'f1_train', 'f1':f1_train})\n",
    "    data.append({'C':num_groups, 'method':method, 'metric':'f1_val_test', 'f1':f1_val_test})\n",
    "\n",
    "f1_data = pd.DataFrame(data)\n",
    "f1_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# format for publication\n",
    "tmp = f1_data.groupby(['C','method','metric'])['f1'].agg(['mean', 'std']).reset_index()\n",
    "tmp['f1'] = tmp.apply(lambda x: f\"{x['mean']:.2f}$\\pm${x['std']:.2f}\", axis=1)\n",
    "tmp['index'] = tmp['C'].astype(str) + ':' + tmp['metric']\n",
    "tmp = tmp[tmp['metric']!='f1_overall']\n",
    "tmp = tmp.pivot(index='index',columns='method',values='f1').reset_index()\n",
    "tmp = tmp.sort_values(by='index')\n",
    "tmp['C'] = tmp['index'].str.split(':').str[0]\n",
    "tmp['metric'] = tmp['index'].str.split(':').str[1]\n",
    "tmp = tmp[['metric','C','ID','OD','SHD','SID','observational','interventional']]\n",
    "tmp.to_csv(f'results-accuracycausalgroups-{timestamp}.csv', index=None)\n",
    "tmp"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
