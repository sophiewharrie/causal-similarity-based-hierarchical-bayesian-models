{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocess WEO data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weo_data = pd.read_excel('WEOApr2023all.xlsx', sheet_name='WEOApr2023all').replace('--',None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract data between given dates for a specific list of variables\n",
    "\n",
    "START_YEAR = 1990\n",
    "END_YEAR = 2019\n",
    "VARIABLES = ['ISO','year','GGX_NGDP','NGDPD','LP','PCPI']\n",
    "\n",
    "k = list(range(1980,2021)) \n",
    "k.append('ISO')\n",
    "k.append('WEO Subject Code')\n",
    "df = weo_data[k].melt(id_vars=['WEO Subject Code','ISO'],var_name='year').dropna()\n",
    "df = df.pivot(index=['ISO','year'],columns='WEO Subject Code',values='value').reset_index()\n",
    "\n",
    "df = df[VARIABLES]\n",
    "\n",
    "df = df.dropna()\n",
    "df = df[(df['year']>=START_YEAR)&(df['year']<=END_YEAR)]\n",
    "\n",
    "df2 = df.groupby('ISO').count().reset_index()[['ISO','year']]\n",
    "COUNTRIES = df2[df2['year']==END_YEAR-START_YEAR+1]['ISO'].tolist()\n",
    "\n",
    "df = df[df['ISO'].isin(COUNTRIES)]\n",
    "\n",
    "iso = df['ISO']\n",
    "weo_data = df.drop(columns='ISO').replace(',','').astype(float)\n",
    "weo_data['ISO'] = iso\n",
    "\n",
    "weo_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocess PWT data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pwt_data = pd.read_excel('pwt1001.xlsx', sheet_name='Data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract data between given dates for a specific list of variables\n",
    "\n",
    "START_YEAR = 1990\n",
    "END_YEAR = 2019\n",
    "VARIABLES = ['countrycode','country','year','emp','hc']\n",
    "\n",
    "df = pwt_data[VARIABLES].dropna()\n",
    "df = df[(df['year']>=START_YEAR)&(df['year']<=END_YEAR)]\n",
    "\n",
    "df2 = df.groupby('countrycode').count().reset_index()[['countrycode','country']]\n",
    "COUNTRIES = df2[df2['country']==END_YEAR-START_YEAR+1]['countrycode'].tolist()\n",
    "\n",
    "df = df[df['countrycode'].isin(COUNTRIES)]\n",
    "pwt_data = df.rename(columns={'countrycode':'ISO'}).drop(columns='country')\n",
    "pwt_data['year'] = pwt_data['year'].astype(float)\n",
    "pwt_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Merge datasets and reformat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.merge(weo_data, pwt_data, on=['ISO','year'])\n",
    "data = data.rename(columns={'NGDPD':'Y','LP':'X_population','emp':'X_employment','hc':'X_human_capital_index','PCPI':'X_inflation_rate','GGX_NGDP':'T_government_expenditure'})\n",
    "\n",
    "# create binary intervention\n",
    "# 1 if government expenditure (as % of GDP) is higher than 30%, 0 otherwise\n",
    "data['T_government_expenditure'] = (data['T_government_expenditure']>30).astype(int)\n",
    "data = data.dropna()\n",
    "\n",
    "# if inflation rate >1000 set it to 1000 (to avoid extreme outliers)\n",
    "data['X_inflation_rate'] = data['X_inflation_rate'].apply(lambda x : x if x<1000 else 1000)\n",
    "\n",
    "# the scales vary widely across variables so we rescale the data to have a common scale across variabless\n",
    "scaler = StandardScaler()\n",
    "exclude_cols = ['ISO','year','T_government_expenditure']\n",
    "data_to_scale = data.drop(columns=exclude_cols)\n",
    "data = pd.concat([data[exclude_cols],pd.DataFrame(scaler.fit_transform(data_to_scale), columns=data_to_scale.keys())], axis=1)\n",
    "\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create random splits for main experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUMTASKS_TRAIN = 49\n",
    "NUMTASKS_VAL = 10\n",
    "NUMTASKS_TEST = 10\n",
    "NUMSAMPLES_SPT = 15\n",
    "NUMSAMPLES_QRY = 15\n",
    "\n",
    "COUNTRIES = data['ISO'].unique().tolist()\n",
    "\n",
    "for dataset in range(1,4):\n",
    "\n",
    "    random.shuffle(COUNTRIES)\n",
    "    train_tasks = COUNTRIES[0:NUMTASKS_TRAIN]\n",
    "    val_tasks = COUNTRIES[NUMTASKS_TRAIN:NUMTASKS_TRAIN+NUMTASKS_VAL]\n",
    "    test_tasks = COUNTRIES[NUMTASKS_TRAIN+NUMTASKS_VAL:]\n",
    "\n",
    "    meta_train_type = {'train':0, 'val':1, 'test':2}\n",
    "    country_list = {'train':train_tasks, 'val':val_tasks, 'test':test_tasks}\n",
    "    \n",
    "    country_map = dict(zip(COUNTRIES,range(len(COUNTRIES))))\n",
    "    \n",
    "    df = data.sample(frac=1)\n",
    "    new_df = pd.DataFrame()\n",
    "    for datatype in ['train', 'val', 'test']:\n",
    "        add_data = df[df['ISO'].isin(country_list[datatype])]\n",
    "        add_data['meta_train'] = meta_train_type[datatype]\n",
    "        add_data['task_train'] = (add_data.groupby('ISO').cumcount() < NUMSAMPLES_SPT).astype(int)\n",
    "        add_data['task'] = add_data['ISO'].map(country_map)\n",
    "        new_df = pd.concat([new_df, add_data[['Y','X_employment','X_human_capital_index','X_population','X_inflation_rate','T_government_expenditure','meta_train','task_train','task']]])\n",
    "\n",
    "    new_df = new_df.sort_values(by=['task','task_train'])\n",
    "    new_df.to_csv(f'econ_dataset{dataset}.csv', index=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create random splits for distribution shift experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUMTASKS_TRAIN = 49\n",
    "NUMTASKS_VAL = 10\n",
    "NUMTASKS_TEST = 10\n",
    "NUMSAMPLES_SPT = 15\n",
    "NUMSAMPLES_QRY = 15\n",
    "\n",
    "COUNTRIES_IN_ORDER_OF_POP = data.groupby('ISO').mean().sort_values(by='X_population', ascending=False).reset_index()['ISO'].tolist()\n",
    "HIGHEST_POP_COUNTRIES = COUNTRIES_IN_ORDER_OF_POP[0:NUMTASKS_TEST] # highest population countries (fixed across all sets)\n",
    "OTHER_COUNTRIES = list(set(COUNTRIES_IN_ORDER_OF_POP) - set(HIGHEST_POP_COUNTRIES))\n",
    "\n",
    "for dataset in range(1,4):\n",
    "\n",
    "    random.shuffle(OTHER_COUNTRIES)\n",
    "    train_tasks = OTHER_COUNTRIES[0:NUMTASKS_TRAIN]\n",
    "    val_tasks = OTHER_COUNTRIES[NUMTASKS_TRAIN:NUMTASKS_TRAIN+NUMTASKS_VAL]\n",
    "    test_tasks = HIGHEST_POP_COUNTRIES # fixed across all sets\n",
    "\n",
    "    meta_train_type = {'train':0, 'val':1, 'test':2}\n",
    "    country_list = {'train':train_tasks, 'val':val_tasks, 'test':test_tasks}\n",
    "    \n",
    "    COUNTRIES = OTHER_COUNTRIES + HIGHEST_POP_COUNTRIES\n",
    "    country_map = dict(zip(COUNTRIES,range(len(COUNTRIES))))\n",
    "    \n",
    "    df = data.sample(frac=1)\n",
    "    new_df = pd.DataFrame()\n",
    "    for datatype in ['train', 'val', 'test']:\n",
    "        add_data = df[df['ISO'].isin(country_list[datatype])]\n",
    "        add_data['meta_train'] = meta_train_type[datatype]\n",
    "        add_data['task_train'] = (add_data.groupby('ISO').cumcount() < NUMSAMPLES_SPT).astype(int)\n",
    "        add_data['task'] = add_data['ISO'].map(country_map)\n",
    "        new_df = pd.concat([new_df, add_data[['Y','X_employment','X_human_capital_index','X_population','X_inflation_rate','T_government_expenditure','meta_train','task_train','task']]])\n",
    "\n",
    "    new_df = new_df.sort_values(by=['task','task_train'])\n",
    "    new_df.to_csv(f'econshift_dataset{dataset}.csv', index=None)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base_ml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
